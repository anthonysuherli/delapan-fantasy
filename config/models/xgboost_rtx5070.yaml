name: "XGBoost RTX 5070 Configuration"
description: "Optimized config for NVIDIA RTX 5070 (16GB VRAM, 6144 CUDA cores)"
version: "1.0.0"

# RTX 5070 Specifications:
# - 16GB GDDR7 VRAM
# - 6144 CUDA cores
# - Ada Lovelace architecture with 4th gen Tensor Cores
# - High memory bandwidth (448 GB/s)
# - Optimized for medium-to-large datasets with aggressive parallelism

model:
  type: "xgboost"
  params:
    # Tree structure - balanced depth for RTX 5070's compute capacity
    max_depth: 8
    learning_rate: 0.05
    n_estimators: 400
    min_child_weight: 3

    # Sampling ratios - moderate values for stability
    subsample: 0.85
    colsample_bytree: 0.85
    colsample_bylevel: 0.8
    colsample_bynode: 0.8

    # Objective and regularization
    objective: "reg:squarederror"
    random_state: 42
    reg_alpha: 0.1
    reg_lambda: 1.0

    # GPU-specific parameters (XGBoost 2.0+ syntax)
    tree_method: "hist"
    device: "cuda:0"
    max_bin: 384

    # Early stopping (disabled - set validation data in training code to enable)
    # early_stopping_rounds: 50

    # Verbosity
    verbosity: 1

# Sample commands for backtesting with RTX 5070:
#
# Single-day backtest:
# python scripts/run_backtest_gpu.py --test-start 20250205 --test-end 20250206 --model-config config/models/xgboost_rtx5070.yaml --per-player --gpu-id 0
#
# Week-long backtest:
# python scripts/run_backtest_gpu.py --test-start 20250201 --test-end 20250207 --model-config config/models/xgboost_rtx5070.yaml --per-player --gpu-id 0
#
# Month-long backtest with recalibration:
# python scripts/run_backtest_gpu.py --test-start 20250201 --test-end 20250228 --model-config config/models/xgboost_rtx5070.yaml --per-player --recalibrate-days 7 --gpu-id 0
#
# Custom training start with validation:
# python scripts/run_backtest_gpu.py --train-start 20241101 --train-end 20250131 --test-start 20250201 --test-end 20250215 --model-config config/models/xgboost_rtx5070.yaml --per-player --gpu-id 0
#
# Monitoring GPU utilization during training:
# nvidia-smi -l 1
#
# Notes:
# - RTX 5070 has 16GB VRAM, can handle 500+ per-player models per slate
# - max_bin=384 balances memory usage and histogram precision
# - Recommended batch size: Process 50-100 players at a time to maximize throughput
# - Expected training time: 10-15 minutes per slate (500 players, 400 trees each)
