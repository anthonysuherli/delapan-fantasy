{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Data Loading Demo\n",
    "\n",
    "This notebook demonstrates the optimized data loading capabilities using TensorFlow and PyTorch backends.\n",
    "\n",
    "Features:\n",
    "- Parallel file reading\n",
    "- Prefetching for overlapping I/O and computation\n",
    "- GPU-ready tensor outputs\n",
    "- Automatic backend selection\n",
    "- Performance comparison with original loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data.storage.parquet_storage import ParquetStorage\n",
    "from src.data.loaders.historical_loader import HistoricalDataLoader\n",
    "from src.data.loaders.optimized_historical_loader import OptimizedHistoricalDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '20250101'\n",
    "END_DATE = '20250131'\n",
    "\n",
    "storage = ParquetStorage(base_dir='../data/inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original Loader (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Original HistoricalDataLoader (Sequential)...\")\n",
    "\n",
    "original_loader = HistoricalDataLoader(storage)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "df_original = original_loader.load_historical_player_logs(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE\n",
    ")\n",
    "original_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"\\nOriginal Loader:\")\n",
    "print(f\"  Rows loaded: {len(df_original):,}\")\n",
    "print(f\"  Time: {original_time:.2f}s\")\n",
    "print(f\"  Throughput: {len(df_original) / original_time:,.0f} rows/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow Backend (Parallel + Prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing OptimizedHistoricalDataLoader with TensorFlow backend...\")\n",
    "\n",
    "try:\n",
    "    tf_loader = OptimizedHistoricalDataLoader(\n",
    "        storage,\n",
    "        loader_type='tensorflow',\n",
    "        num_workers=8,\n",
    "        enable_prefetch=True,\n",
    "        enable_cache=True\n",
    "    )\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    df_tf = tf_loader.load_historical_player_logs(\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE\n",
    "    )\n",
    "    tf_time = time.perf_counter() - start_time\n",
    "    \n",
    "    print(f\"\\nTensorFlow Loader:\")\n",
    "    print(f\"  Rows loaded: {len(df_tf):,}\")\n",
    "    print(f\"  Time: {tf_time:.2f}s\")\n",
    "    print(f\"  Throughput: {len(df_tf) / tf_time:,.0f} rows/second\")\n",
    "    print(f\"  Speedup: {original_time / tf_time:.2f}x\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"TensorFlow not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Backend (Multi-worker DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing OptimizedHistoricalDataLoader with PyTorch backend...\")\n",
    "\n",
    "try:\n",
    "    pytorch_loader = OptimizedHistoricalDataLoader(\n",
    "        storage,\n",
    "        loader_type='pytorch',\n",
    "        num_workers=8,\n",
    "        enable_prefetch=True,\n",
    "        enable_cache=True\n",
    "    )\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    df_pytorch = pytorch_loader.load_historical_player_logs(\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE\n",
    "    )\n",
    "    pytorch_time = time.perf_counter() - start_time\n",
    "    \n",
    "    print(f\"\\nPyTorch Loader:\")\n",
    "    print(f\"  Rows loaded: {len(df_pytorch):,}\")\n",
    "    print(f\"  Time: {pytorch_time:.2f}s\")\n",
    "    print(f\"  Throughput: {len(df_pytorch) / pytorch_time:,.0f} rows/second\")\n",
    "    print(f\"  Speedup: {original_time / pytorch_time:.2f}x\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"PyTorch not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto Backend (Automatic Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing OptimizedHistoricalDataLoader with auto backend selection...\")\n",
    "\n",
    "auto_loader = OptimizedHistoricalDataLoader(\n",
    "    storage,\n",
    "    loader_type='auto',\n",
    "    num_workers=8,\n",
    "    enable_prefetch=True,\n",
    "    enable_cache=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSelected backend: {auto_loader.active_backend}\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "df_auto = auto_loader.load_historical_player_logs(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE\n",
    ")\n",
    "auto_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"\\nAuto Loader:\")\n",
    "print(f\"  Rows loaded: {len(df_auto):,}\")\n",
    "print(f\"  Time: {auto_time:.2f}s\")\n",
    "print(f\"  Throughput: {len(df_auto) / auto_time:,.0f} rows/second\")\n",
    "print(f\"  Speedup: {original_time / auto_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Loader': ['Original', 'TensorFlow', 'PyTorch'],\n",
    "    'Time (s)': [original_time, tf_time, pytorch_time],\n",
    "    'Throughput (rows/s)': [\n",
    "        len(df_original) / original_time,\n",
    "        len(df_tf) / tf_time,\n",
    "        len(df_pytorch) / pytorch_time\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "results.plot(x='Loader', y='Time (s)', kind='bar', ax=ax1, legend=False, color='steelblue')\n",
    "ax1.set_title('Load Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_xlabel('')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "results.plot(x='Loader', y='Throughput (rows/s)', kind='bar', ax=ax2, legend=False, color='coral')\n",
    "ax2.set_title('Throughput Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Rows per second')\n",
    "ax2.set_xlabel('')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Optimized Loader in Walk-Forward Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example: Replace loader in WalkForwardBacktest\")\n",
    "print()\n",
    "print(\"Original code:\")\n",
    "print(\"  storage = SQLiteStorage(db_path)\")\n",
    "print(\"  loader = HistoricalDataLoader(storage)\")\n",
    "print()\n",
    "print(\"Optimized code:\")\n",
    "print(\"  storage = ParquetStorage(base_dir='data/inputs')\")\n",
    "print(\"  loader = OptimizedHistoricalDataLoader(\")\n",
    "print(\"      storage,\")\n",
    "print(\"      loader_type='auto',  # or 'tensorflow' / 'pytorch'\")\n",
    "print(\"      num_workers=8,\")\n",
    "print(\"      enable_prefetch=True,\")\n",
    "print(\"      enable_cache=True\")\n",
    "print(\"  )\")\n",
    "print()\n",
    "print(\"The OptimizedHistoricalDataLoader is a drop-in replacement.\")\n",
    "print(\"All existing methods work identically but with improved performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: TensorFlow Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.data.loaders.tensorflow_loader import TensorFlowDataLoader\n",
    "    \n",
    "    print(\"Creating optimized TensorFlow dataset for model training...\")\n",
    "    \n",
    "    tf_data_loader = TensorFlowDataLoader(\n",
    "        prefetch_buffer_size='AUTOTUNE',\n",
    "        num_parallel_reads=8,\n",
    "        cache=True\n",
    "    )\n",
    "    \n",
    "    feature_cols = [col for col in df_tf.columns if col.startswith(('rolling_', 'ewma_'))]\n",
    "    \n",
    "    if feature_cols:\n",
    "        dataset = tf_data_loader.create_cached_dataset(\n",
    "            data=df_tf.head(10000),\n",
    "            feature_columns=feature_cols[:50],\n",
    "            target_column='fpts',\n",
    "            batch_size=32,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset created:\")\n",
    "        print(f\"  Features: {len(feature_cols[:50])}\")\n",
    "        print(f\"  Batch size: 32\")\n",
    "        print(f\"  Prefetching: AUTOTUNE\")\n",
    "        print(f\"  Caching: Enabled\")\n",
    "        print(f\"\\nDataset is ready for model.fit() with optimized I/O\")\n",
    "    else:\n",
    "        print(\"No feature columns found. Run feature engineering first.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced: PyTorch DataLoader for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.data.loaders.pytorch_loader import PyTorchDataLoader, ParquetDataset\n",
    "    \n",
    "    print(\"Creating optimized PyTorch DataLoader for model training...\")\n",
    "    \n",
    "    pytorch_data_loader = PyTorchDataLoader(\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    feature_cols = [col for col in df_pytorch.columns if col.startswith(('rolling_', 'ewma_'))]\n",
    "    \n",
    "    if feature_cols:\n",
    "        dataset = pytorch_data_loader.create_dataset_from_dataframe(\n",
    "            data=df_pytorch.head(10000),\n",
    "            feature_columns=feature_cols[:50],\n",
    "            target_column='fpts',\n",
    "            cache_in_memory=True\n",
    "        )\n",
    "        \n",
    "        dataloader = pytorch_data_loader.create_dataloader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataLoader created:\")\n",
    "        print(f\"  Features: {len(feature_cols[:50])}\")\n",
    "        print(f\"  Batch size: 32\")\n",
    "        print(f\"  Workers: 4\")\n",
    "        print(f\"  Pin memory: True\")\n",
    "        print(f\"\\nDataLoader is ready for training loop with optimized multi-worker loading\")\n",
    "    else:\n",
    "        print(\"No feature columns found. Run feature engineering first.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Drop-in replacement**: OptimizedHistoricalDataLoader works identically to HistoricalDataLoader\n",
    "2. **Automatic backend selection**: Use `loader_type='auto'` for convenience\n",
    "3. **Parallel file reading**: Multiple files loaded concurrently\n",
    "4. **Prefetching**: I/O and computation overlap for better pipeline utilization\n",
    "5. **GPU-ready**: Pin memory enabled for faster GPU transfer\n",
    "6. **Advanced usage**: TensorFlow/PyTorch datasets for model training with optimized batching\n",
    "\n",
    "Expected speedup: 2-5x depending on file count and system I/O capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
