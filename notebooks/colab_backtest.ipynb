{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA DFS Walk-Forward Backtest - Google Colab\n",
    "\n",
    "This notebook runs per-player model training on Google Colab with Google Drive persistence.\n",
    "\n",
    "## Setup Requirements\n",
    "1. Upload your data to Google Drive: `MyDrive/nba_dfs/data/inputs/`\n",
    "2. Run all cells in order\n",
    "3. Results saved to: `MyDrive/nba_dfs/outputs/`\n",
    "\n",
    "## Estimated Time\n",
    "- Free Colab: ~21 min per slate\n",
    "- Colab Pro: ~10 min per slate\n",
    "- Colab Pro+: ~5 min per slate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/nba_dfs')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost==1.7.6 pyarrow fastparquet pyyaml python-dotenv joblib scipy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Structure\n",
    "\n",
    "First time only: Upload your project files to Drive or clone from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path('/content/drive/MyDrive/nba_dfs')\n",
    "\n",
    "if not project_root.exists():\n",
    "    print(\"Creating project structure...\")\n",
    "    project_root.mkdir(parents=True, exist_ok=True)\n",
    "    (project_root / 'data' / 'inputs').mkdir(parents=True, exist_ok=True)\n",
    "    (project_root / 'data' / 'outputs').mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Project structure created. Please upload your src/ and config/ folders.\")\n",
    "else:\n",
    "    print(f\"Project exists at {project_root}\")\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Python path: {sys.path[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = project_root / 'data' / 'inputs'\n",
    "\n",
    "print(\"Data directories:\")\n",
    "for subdir in ['box_scores', 'dfs_salaries', 'betting_odds', 'schedule']:\n",
    "    path = data_dir / subdir\n",
    "    if path.exists():\n",
    "        count = len(list(path.glob('*.parquet')))\n",
    "        print(f\"  {subdir}: {count} files\")\n",
    "    else:\n",
    "        print(f\"  {subdir}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check System Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import multiprocessing\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "\n",
    "print(f\"CPU Cores: {cpu_count}\")\n",
    "print(f\"RAM: {ram_gb:.1f} GB\")\n",
    "print(f\"Recommended n_jobs: {cpu_count}\")\n",
    "\n",
    "if ram_gb < 12:\n",
    "    print(\"WARNING: Low RAM detected. Consider reducing n_jobs or processing fewer players.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Backtest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START = '20241001'\n",
    "TRAIN_END = '20241130'\n",
    "TEST_START = '20241201'\n",
    "TEST_END = '20241215'\n",
    "\n",
    "MODEL_TYPE = 'xgboost'\n",
    "FEATURE_CONFIG = 'default_features'\n",
    "MIN_PLAYER_GAMES = 10\n",
    "RECALIBRATE_DAYS = 7\n",
    "\n",
    "N_JOBS = -1\n",
    "\n",
    "DB_PATH = str(project_root / 'nba_dfs.db')\n",
    "OUTPUT_DIR = str(project_root / 'data' / 'outputs')\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Training: {TRAIN_START} to {TRAIN_END}\")\n",
    "print(f\"  Testing: {TEST_START} to {TEST_END}\")\n",
    "print(f\"  Model: {MODEL_TYPE}\")\n",
    "print(f\"  Features: {FEATURE_CONFIG}\")\n",
    "print(f\"  Parallel jobs: {N_JOBS} (all cores)\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from src.walk_forward_backtest import WalkForwardBacktest\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "backtest = WalkForwardBacktest(\n",
    "    db_path=DB_PATH,\n",
    "    train_start=TRAIN_START,\n",
    "    train_end=TRAIN_END,\n",
    "    test_start=TEST_START,\n",
    "    test_end=TEST_END,\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_params=model_params,\n",
    "    feature_config=FEATURE_CONFIG,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_player_models=True,\n",
    "    min_player_games=MIN_PLAYER_GAMES,\n",
    "    recalibrate_days=RECALIBRATE_DAYS,\n",
    "    save_models=True,\n",
    "    save_predictions=True,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(\"Backtest initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Backtest\n",
    "\n",
    "This will take 5-21 minutes per slate depending on your Colab tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f\"Starting backtest at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = backtest.run()\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed = end_time - start_time\n",
    "print(\"=\"*80)\n",
    "print(f\"Backtest completed at {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BACKTEST RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Slates processed: {results['num_slates']}\")\n",
    "print(f\"Date range: {results['date_range']}\")\n",
    "print(f\"Total players evaluated: {results['total_players_evaluated']:.0f}\")\n",
    "print()\n",
    "print(\"Model Performance:\")\n",
    "print(f\"  Mean MAPE: {results['model_mean_mape']:.2f}%\")\n",
    "print(f\"  Median MAPE: {results['model_median_mape']:.2f}%\")\n",
    "print(f\"  Mean RMSE: {results['model_mean_rmse']:.2f}\")\n",
    "print(f\"  Mean MAE: {results['model_mean_mae']:.2f}\")\n",
    "print(f\"  Mean Correlation: {results['model_mean_correlation']:.3f}\")\n",
    "print()\n",
    "print(\"Benchmark Performance:\")\n",
    "print(f\"  Mean MAPE: {results['benchmark_mean_mape']:.2f}%\")\n",
    "print(f\"  Improvement: {results['mape_improvement']:+.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. View Daily Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = results['daily_results']\n",
    "display(daily_df[[\n",
    "    'date', 'num_players', 'model_mape', 'model_rmse', \n",
    "    'model_corr', 'benchmark_mape', 'mean_actual', 'mean_projected'\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(daily_df['date'], daily_df['model_mape'], marker='o', label='Model')\n",
    "axes[0, 0].plot(daily_df['date'], daily_df['benchmark_mape'], marker='s', label='Benchmark')\n",
    "axes[0, 0].set_title('MAPE by Date')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('MAPE (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[0, 1].plot(daily_df['date'], daily_df['model_corr'], marker='o', color='green')\n",
    "axes[0, 1].set_title('Correlation by Date')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Correlation')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1, 0].bar(range(len(daily_df)), daily_df['num_players'])\n",
    "axes[1, 0].set_title('Players Evaluated per Slate')\n",
    "axes[1, 0].set_xlabel('Slate Index')\n",
    "axes[1, 0].set_ylabel('Number of Players')\n",
    "\n",
    "all_preds = results['all_predictions']\n",
    "if not all_preds.empty:\n",
    "    axes[1, 1].scatter(all_preds['actual_fpts'], all_preds['projected_fpts'], alpha=0.5)\n",
    "    axes[1, 1].plot([0, 70], [0, 70], 'r--', label='Perfect Prediction')\n",
    "    axes[1, 1].set_title('Actual vs Projected Fantasy Points')\n",
    "    axes[1, 1].set_xlabel('Actual FPts')\n",
    "    axes[1, 1].set_ylabel('Projected FPts')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'report_path' in results:\n",
    "    print(f\"Report saved to: {results['report_path']}\")\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {backtest.run_output_dir}\")\n",
    "print(f\"  - Predictions: {backtest.run_predictions_dir}\")\n",
    "print(f\"  - Training inputs: {backtest.run_inputs_dir}\")\n",
    "\n",
    "output_files = list(Path(backtest.run_output_dir).rglob('*'))\n",
    "print(f\"\\nTotal output files: {len(output_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Summary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = project_root / 'data' / 'outputs' / f\"summary_{results['date_range'].replace(' to ', '_')}.csv\"\n",
    "daily_df.to_csv(summary_path, index=False)\n",
    "print(f\"Summary CSV saved to: {summary_path}\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(str(summary_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "archive_path = project_root / 'data' / 'outputs' / f\"backtest_results_{backtest.run_timestamp}.zip\"\n",
    "shutil.make_archive(\n",
    "    str(archive_path.with_suffix('')),\n",
    "    'zip',\n",
    "    backtest.run_output_dir\n",
    ")\n",
    "\n",
    "print(f\"Results archived to: {archive_path}\")\n",
    "print(f\"Size: {archive_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "\n",
    "files.download(str(archive_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
