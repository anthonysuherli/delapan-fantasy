{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Day Slate-Level Backtest with Benchmark Comparison\n",
    "\n",
    "This notebook trains a single slate-level XGBoost model and compares against season average benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data\n",
    "from src.data.loaders.historical_loader import HistoricalDataLoader\n",
    "from src.data.storage.parquet_storage import ParquetStorage\n",
    "\n",
    "# Features\n",
    "from src.features.pipeline import FeaturePipeline\n",
    "from src.utils.feature_config import load_feature_config\n",
    "\n",
    "# Models\n",
    "from src.models.xgboost_model import XGBoostModel\n",
    "\n",
    "# Evaluation\n",
    "from src.evaluation.benchmarks.season_average import SeasonAverageBenchmark\n",
    "\n",
    "print('Imports complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates\n",
    "PREDICTION_DATE = '20250115'\n",
    "TRAIN_START = '20241001'\n",
    "TRAIN_END = '20250114'\n",
    "\n",
    "# Model config\n",
    "XGBOOST_CONFIG = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Salary tiers\n",
    "SALARY_TIERS = [0, 4000, 6000, 8000, 15000]\n",
    "\n",
    "print(f\"Configuration set for slate prediction on {PREDICTION_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = ParquetStorage()\n",
    "loader = HistoricalDataLoader(storage)\n",
    "\n",
    "# Load test slate\n",
    "test_slate = loader.load_slate_data(PREDICTION_DATE)\n",
    "print(f\"Test slate: {len(test_slate)} players\")\n",
    "\n",
    "# Load training data\n",
    "train_data = loader.load_historical_data(TRAIN_START, TRAIN_END)\n",
    "print(f\"Training data: {len(train_data)} records from {train_data['playerID'].nunique()} players\")\n",
    "\n",
    "# Date range check\n",
    "print(f\"\\nTraining date range: {train_data['gameDate'].min()} to {train_data['gameDate'].max()}\")\n",
    "print(f\"Test date: {PREDICTION_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Fit Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark using training data\n",
    "benchmark = SeasonAverageBenchmark(min_games=5)\n",
    "benchmark.fit(train_data)\n",
    "\n",
    "print(f\"Benchmark fitted for {len(benchmark.player_averages)} players\")\n",
    "\n",
    "# Generate benchmark predictions\n",
    "test_slate['benchmark_pred'] = test_slate['playerID'].map(benchmark.player_averages).fillna(0)\n",
    "\n",
    "# Coverage\n",
    "has_benchmark = test_slate['benchmark_pred'] > 0\n",
    "print(f\"Benchmark coverage: {has_benchmark.sum()}/{len(test_slate)} ({has_benchmark.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature configuration\n",
    "feature_config = load_feature_config('default_features')\n",
    "pipeline = feature_config.build_pipeline(FeaturePipeline)\n",
    "\n",
    "# Generate training features\n",
    "print(\"Generating training features...\")\n",
    "train_features = pipeline.fit_transform(train_data)\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = [col for col in train_features.columns \n",
    "               if col not in ['playerID', 'gameDate', 'fpts', 'playerName']]\n",
    "\n",
    "print(f\"Generated {len(feature_cols)} features\")\n",
    "print(f\"Training shape: {train_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Slate-Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train = train_features[feature_cols]\n",
    "y_train = train_features['fpts']\n",
    "\n",
    "# Remove NaN values\n",
    "mask = ~(X_train.isna().any(axis=1) | y_train.isna())\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(f\"Training samples after cleaning: {len(X_train)}\")\n",
    "\n",
    "# Train model\n",
    "slate_model = XGBoostModel(XGBOOST_CONFIG)\n",
    "slate_model.train(X_train, y_train)\n",
    "\n",
    "print(\"Slate-level model trained\")\n",
    "\n",
    "# Feature importance\n",
    "importance = slate_model.get_feature_importance()\n",
    "top_features = sorted(zip(feature_cols, importance), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 features:\")\n",
    "for feat, imp in top_features:\n",
    "    print(f\"  {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data for test players\n",
    "test_player_history = loader.load_historical_player_logs(PREDICTION_DATE, lookback_days=365)\n",
    "\n",
    "# Filter to test slate players\n",
    "test_players = test_slate['playerID'].unique()\n",
    "test_player_history = test_player_history[test_player_history['playerID'].isin(test_players)]\n",
    "\n",
    "print(f\"Test player history: {len(test_player_history)} records\")\n",
    "\n",
    "# Generate features for test data\n",
    "test_features = pipeline.transform(test_player_history)\n",
    "\n",
    "# Get most recent features for each player\n",
    "latest_features = test_features.sort_values('gameDate').groupby('playerID').last()\n",
    "\n",
    "print(f\"Test features for {len(latest_features)} players\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "test_slate_features = test_slate.set_index('playerID').join(latest_features[feature_cols], how='left')\n",
    "\n",
    "# Generate predictions\n",
    "X_test = test_slate_features[feature_cols]\n",
    "has_features = ~X_test.isna().all(axis=1)\n",
    "\n",
    "# Initialize predictions\n",
    "test_slate['model_pred'] = 0\n",
    "\n",
    "# Predict for players with features\n",
    "if has_features.sum() > 0:\n",
    "    X_valid = X_test[has_features].fillna(0)\n",
    "    predictions = slate_model.predict(X_valid)\n",
    "    test_slate.loc[has_features, 'model_pred'] = predictions\n",
    "\n",
    "print(f\"Model predictions for {has_features.sum()}/{len(test_slate)} players\")\n",
    "\n",
    "# Preview predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "sample = test_slate[['playerName', 'salary', 'model_pred', 'benchmark_pred', 'fpts']].head(10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model vs Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to players with both predictions\n",
    "has_both = (test_slate['model_pred'] > 0) & (test_slate['benchmark_pred'] > 0)\n",
    "comparison_data = test_slate[has_both].copy()\n",
    "\n",
    "print(f\"Comparing {len(comparison_data)} players with both predictions\")\n",
    "\n",
    "# Overall comparison\n",
    "comparison = benchmark.compare_with_model(\n",
    "    actual=comparison_data['fpts'],\n",
    "    model_pred=comparison_data['model_pred'],\n",
    "    benchmark_pred=comparison_data['benchmark_pred']\n",
    ")\n",
    "\n",
    "print(comparison['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary Tier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare by salary tier\n",
    "tier_comparison = benchmark.compare_by_salary_tier(comparison_data, SALARY_TIERS)\n",
    "\n",
    "print(\"Performance by Salary Tier:\")\n",
    "print(\"=\" * 100)\n",
    "print(tier_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize which tiers benefit from the model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Model Performance vs Benchmark by Tier:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in tier_comparison.iterrows():\n",
    "    mape_imp = row['mape_improvement']\n",
    "    rmse_imp = row['rmse_improvement']\n",
    "    \n",
    "    # Determine overall performance\n",
    "    if mape_imp > 0 and rmse_imp > 0:\n",
    "        overall = \"BETTER\"\n",
    "    elif mape_imp < 0 and rmse_imp < 0:\n",
    "        overall = \"WORSE\"\n",
    "    else:\n",
    "        overall = \"MIXED\"\n",
    "    \n",
    "    print(f\"{row['salary_tier']:20} {overall:8} | \"\n",
    "          f\"MAPE: {mape_imp:+6.1f}% | RMSE: {rmse_imp:+6.2f} pts | \"\n",
    "          f\"N={row['n_players']:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "comparison_data['model_error'] = comparison_data['model_pred'] - comparison_data['fpts']\n",
    "comparison_data['benchmark_error'] = comparison_data['benchmark_pred'] - comparison_data['fpts']\n",
    "comparison_data['model_abs_error'] = np.abs(comparison_data['model_error'])\n",
    "comparison_data['benchmark_abs_error'] = np.abs(comparison_data['benchmark_error'])\n",
    "\n",
    "# Error statistics\n",
    "print(\"Error Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModel Errors:\")\n",
    "print(f\"  Mean Error (bias): {comparison_data['model_error'].mean():.2f} pts\")\n",
    "print(f\"  Std Error: {comparison_data['model_error'].std():.2f} pts\")\n",
    "print(f\"  Median Abs Error: {comparison_data['model_abs_error'].median():.2f} pts\")\n",
    "print(f\"  95th Percentile Abs Error: {comparison_data['model_abs_error'].quantile(0.95):.2f} pts\")\n",
    "\n",
    "print(\"\\nBenchmark Errors:\")\n",
    "print(f\"  Mean Error (bias): {comparison_data['benchmark_error'].mean():.2f} pts\")\n",
    "print(f\"  Std Error: {comparison_data['benchmark_error'].std():.2f} pts\")\n",
    "print(f\"  Median Abs Error: {comparison_data['benchmark_abs_error'].median():.2f} pts\")\n",
    "print(f\"  95th Percentile Abs Error: {comparison_data['benchmark_abs_error'].quantile(0.95):.2f} pts\")\n",
    "\n",
    "# Win rate\n",
    "model_wins = comparison_data['model_abs_error'] < comparison_data['benchmark_abs_error']\n",
    "print(f\"\\nModel Win Rate: {model_wins.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Predictions Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top predicted players\n",
    "TOP_N = 20\n",
    "\n",
    "# Top by model\n",
    "top_model = comparison_data.nlargest(TOP_N, 'model_pred')\n",
    "model_top_actual = top_model['fpts'].sum()\n",
    "model_top_predicted = top_model['model_pred'].sum()\n",
    "\n",
    "# Top by benchmark\n",
    "top_benchmark = comparison_data.nlargest(TOP_N, 'benchmark_pred')\n",
    "benchmark_top_actual = top_benchmark['fpts'].sum()\n",
    "benchmark_top_predicted = top_benchmark['benchmark_pred'].sum()\n",
    "\n",
    "print(f\"Top {TOP_N} Players Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel's Top {TOP_N}:\")\n",
    "print(f\"  Predicted Total: {model_top_predicted:.1f} pts\")\n",
    "print(f\"  Actual Total: {model_top_actual:.1f} pts\")\n",
    "print(f\"  Error: {model_top_predicted - model_top_actual:.1f} pts\")\n",
    "print(f\"  MAPE: {np.mean(np.abs(top_model['model_pred'] - top_model['fpts']) / top_model['fpts']) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nBenchmark's Top {TOP_N}:\")\n",
    "print(f\"  Predicted Total: {benchmark_top_predicted:.1f} pts\")\n",
    "print(f\"  Actual Total: {benchmark_top_actual:.1f} pts\")\n",
    "print(f\"  Error: {benchmark_top_predicted - benchmark_top_actual:.1f} pts\")\n",
    "print(f\"  MAPE: {np.mean(np.abs(top_benchmark['benchmark_pred'] - top_benchmark['fpts']) / top_benchmark['fpts']) * 100:.1f}%\")\n",
    "\n",
    "# Overlap\n",
    "overlap = set(top_model['playerID']) & set(top_benchmark['playerID'])\n",
    "print(f\"\\nOverlap: {len(overlap)}/{TOP_N} players in both top lists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "output_file = f'slate_benchmark_comparison_{PREDICTION_DATE}.csv'\n",
    "comparison_data.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Save summary metrics\n",
    "summary_metrics = pd.DataFrame([\n",
    "    {'metric': 'model_mape', 'value': comparison['model']['mape']},\n",
    "    {'metric': 'benchmark_mape', 'value': comparison['benchmark']['mape']},\n",
    "    {'metric': 'mape_improvement', 'value': comparison['improvement']['mape_improvement']},\n",
    "    {'metric': 'model_rmse', 'value': comparison['model']['rmse']},\n",
    "    {'metric': 'benchmark_rmse', 'value': comparison['benchmark']['rmse']},\n",
    "    {'metric': 'rmse_improvement', 'value': comparison['improvement']['rmse_improvement']},\n",
    "    {'metric': 'model_correlation', 'value': comparison['model']['correlation']},\n",
    "    {'metric': 'benchmark_correlation', 'value': comparison['benchmark']['correlation']},\n",
    "    {'metric': 'model_win_rate', 'value': model_wins.mean()}\n",
    "])\n",
    "\n",
    "summary_file = f'slate_summary_metrics_{PREDICTION_DATE}.csv'\n",
    "summary_metrics.to_csv(summary_file, index=False)\n",
    "print(f\"Summary metrics saved to {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}