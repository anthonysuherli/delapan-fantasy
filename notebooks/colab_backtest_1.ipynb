{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NBA DFS Walk-Forward Backtest - Google Colab\n\nThis notebook runs per-player model training on Google Colab with Bayesian hyperparameter optimization.\n\n## Architecture\n- **Code**: Cloned from GitHub (or synced from Drive)\n- **Data**: Google Drive at `/content/delapan-fantasy/MyDrive/dfs/data/`\n- **Database**: SQLite at drive data directory\n- **Outputs**: Saved to drive data outputs directory with full reports\n\n## Features\n- **Bayesian Optimization**: Optuna-based hyperparameter tuning with TPE sampler\n- **Per-player XGBoost models**: Individual models for each player with parallel training\n- **Season average benchmark**: Statistical comparison with baseline predictions\n- **Statistical testing**: Paired t-test, Cohen's d effect size\n- **Salary tier analysis**: Performance breakdown by player salary ranges\n- **Comprehensive visualizations**: Interactive Plotly charts\n- **Model persistence**: Save models and predictions for reproducibility\n- **Incremental checkpointing**: Resume from interruptions without losing progress\n- **Automatic resume**: Set RESUME_FROM_RUN to continue interrupted backtests\n\n## Resume Capability\nIf Colab disconnects or run is interrupted:\n1. Run cell 8a to check existing runs and their progress\n2. Set RESUME_FROM_RUN to the timestamp (e.g., '20250205_143022')\n3. Re-run from section 9 onwards\n4. Completed slates are skipped automatically\n\n## Setup Requirements\n1. Data synced to Google Drive: `MyDrive/dfs/data/inputs/`\n2. Database populated: `MyDrive/dfs/data/nba_dfs.db`\n3. Run all cells in order\n4. Results saved with detailed reports\n\n## Estimated Time\n- **Bayesian Optimization**: 30 minutes (if enabled)\n- **Per-slate backtest**:\n  - Free Colab: ~21 min per slate\n  - Colab Pro: ~10 min per slate\n  - Colab Pro+: ~5 min per slate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to access data and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/delapan-fantasy')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/delapan-fantasy/MyDrive/delapan-fantasy')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Install required packages including Optuna for Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost==1.7.6 pyarrow fastparquet pyyaml python-dotenv joblib scipy tqdm plotly optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Structure\n",
    "\n",
    "Verify project files exist and add to Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path('/content/delapan-fantasy/MyDrive/delapan-fantasy')\n",
    "\n",
    "if not project_root.exists():\n",
    "    print(\"Creating project structure...\")\n",
    "    project_root.mkdir(parents=True, exist_ok=True)\n",
    "    (project_root / 'data' / 'inputs').mkdir(parents=True, exist_ok=True)\n",
    "    (project_root / 'data' / 'outputs').mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Project structure created. Please upload your src/ and config/ folders.\")\n",
    "else:\n",
    "    print(f\"Project exists at {project_root}\")\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Python path: {sys.path[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Data\n",
    "\n",
    "Check that required data files are present in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('/content/delapan-fantasy/MyDrive/dfs/data')\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "print(\"\\nGames directory:\")\n",
    "games_dir = data_dir / 'games'\n",
    "if games_dir.exists():\n",
    "    count = len(list(games_dir.rglob('*.parquet')))\n",
    "    print(f\"  Total game files: {count}\")\n",
    "else:\n",
    "    print(f\"  games: does not exist\")\n",
    "\n",
    "print(\"\\nInputs directories:\")\n",
    "inputs_dir = data_dir\n",
    "for subdir in ['dfs_salaries', 'betting_odds', 'schedule', 'projections']:\n",
    "    path = inputs_dir / subdir\n",
    "    if path.exists():\n",
    "        count = len(list(path.rglob('*.parquet')))\n",
    "        print(f\"  {subdir}: {count} files\")\n",
    "    else:\n",
    "        print(f\"  {subdir}: does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check System Resources\n",
    "\n",
    "Verify available CPU and RAM for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import multiprocessing\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "\n",
    "print(f\"CPU Cores: {cpu_count}\")\n",
    "print(f\"RAM: {ram_gb:.1f} GB\")\n",
    "print(f\"Recommended n_jobs: {cpu_count}\")\n",
    "\n",
    "if ram_gb < 12:\n",
    "    print(\"WARNING: Low RAM detected. Consider reducing n_jobs or processing fewer players.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6a. Bayesian Optimization Configuration\n\nConfigure Optuna hyperparameter optimization settings.\n\n### Parameters:\n- **RUN_OPTIMIZATION**: Set to `True` to enable Bayesian optimization before backtest\n- **OPTIMIZATION_N_TRIALS**: Number of hyperparameter combinations to try (default: 50)\n- **OPTIMIZATION_CV_FOLDS**: Cross-validation folds for each trial (default: 3)\n- **OPTIMIZATION_SAMPLE_SIZE**: Training samples for optimization (default: 5000)\n  - Larger = more accurate but slower\n  - 5000 provides good balance for 30-minute optimization\n- **OPTIMIZATION_TIMEOUT**: Maximum optimization time in seconds (default: 1800 = 30 min)\n- **OPTIMIZATION_EARLY_STOPPING_PATIENCE**: Stop if no improvement for N trials (default: 10)\n  - Prevents wasting compute on converged hyperparameter space\n  - Tracks best MAPE over sliding window\n\n### How it works:\n1. Loads training data and builds features\n2. Samples records for faster optimization\n3. Uses TPE (Tree-structured Parzen Estimator) sampler\n4. Minimizes MAPE via 3-fold cross-validation\n5. Early stops if no improvement for patience window\n6. Updates MODEL_PARAMS with best hyperparameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "RUN_OPTIMIZATION = False\nOPTIMIZATION_N_TRIALS = 50\nOPTIMIZATION_CV_FOLDS = 3\nOPTIMIZATION_SAMPLE_SIZE = 5000\nOPTIMIZATION_TIMEOUT = 1800\nOPTIMIZATION_EARLY_STOPPING_PATIENCE = 10\n\nprint(\"Bayesian Optimization Configuration:\")\nprint(f\"  Run optimization: {RUN_OPTIMIZATION}\")\nprint(f\"  Number of trials: {OPTIMIZATION_N_TRIALS}\")\nprint(f\"  CV folds: {OPTIMIZATION_CV_FOLDS}\")\nprint(f\"  Sample size: {OPTIMIZATION_SAMPLE_SIZE} records\")\nprint(f\"  Timeout: {OPTIMIZATION_TIMEOUT}s ({OPTIMIZATION_TIMEOUT/60:.1f} min)\")\nprint(f\"  Early stopping patience: {OPTIMIZATION_EARLY_STOPPING_PATIENCE} trials\")\nprint(\"\\nSet RUN_OPTIMIZATION = True to enable hyperparameter tuning\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Define Optimization Objective Function\n",
    "\n",
    "Defines the hyperparameter search space and evaluation metric.\n",
    "\n",
    "### Hyperparameters optimized:\n",
    "- **max_depth**: Tree depth (3-10)\n",
    "- **learning_rate**: Step size shrinkage (0.01-0.3, log scale)\n",
    "- **n_estimators**: Number of boosting rounds (100-500)\n",
    "- **min_child_weight**: Minimum sum of instance weight in child (1-10)\n",
    "- **subsample**: Row sampling ratio (0.6-1.0)\n",
    "- **colsample_bytree**: Column sampling ratio (0.6-1.0)\n",
    "- **gamma**: Minimum loss reduction for split (0.0-5.0)\n",
    "- **reg_alpha**: L1 regularization (0.0-10.0)\n",
    "- **reg_lambda**: L2 regularization (0.0-10.0)\n",
    "\n",
    "### Evaluation:\n",
    "- Uses K-fold cross-validation (default: 3 folds)\n",
    "- Minimizes Mean Absolute Percentage Error (MAPE)\n",
    "- Returns average MAPE across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial, X_sample, y_sample):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object\n",
    "        X_sample: Feature matrix (sampled training data)\n",
    "        y_sample: Target vector (fantasy points)\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean MAPE across CV folds (lower is better)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=OPTIMIZATION_CV_FOLDS, shuffle=True, random_state=42)\n",
    "    mape_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_sample):\n",
    "        X_train_fold = X_sample.iloc[train_idx]\n",
    "        y_train_fold = y_sample.iloc[train_idx]\n",
    "        X_val_fold = X_sample.iloc[val_idx]\n",
    "        y_val_fold = y_sample.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        preds = np.maximum(preds, 0)\n",
    "        \n",
    "        mask = y_val_fold > 0\n",
    "        if mask.sum() > 0:\n",
    "            mape = mean_absolute_percentage_error(y_val_fold[mask], preds[mask]) * 100\n",
    "            mape_scores.append(mape)\n",
    "    \n",
    "    return np.mean(mape_scores) if mape_scores else float('inf')\n",
    "\n",
    "print(\"Objective function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6c. Run Bayesian Optimization\n",
    "\n",
    "Execute Optuna optimization if enabled. This cell:\n",
    "1. Loads training data from database\n",
    "2. Builds features using configured pipeline\n",
    "3. Samples data for efficient optimization\n",
    "4. Runs TPE-based hyperparameter search\n",
    "5. Updates MODEL_PARAMS with best hyperparameters\n",
    "\n",
    "**Note**: This takes approximately 30 minutes with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if RUN_OPTIMIZATION:\n    print(\"Starting Bayesian optimization...\")\n    print(\"=\"*80)\n    \n    from pathlib import Path\n    from src.data.storage.sqlite_storage import SQLiteStorage\n    from src.data.loaders.historical_loader import HistoricalDataLoader\n    from src.utils.feature_config import load_feature_config\n    from src.features.pipeline import FeaturePipeline\n    from src.utils.fantasy_points import calculate_dk_fantasy_points\n    import pandas as pd\n    \n    DATA_DIR = '/content/delapan-fantasy/MyDrive/dfs/data'\n    DB_PATH = 'nba_dfs.db'\n    TRAIN_START = '20241001'\n    TRAIN_END = '20250204'\n    FEATURE_CONFIG = 'default_features'\n    \n    data_path = Path(DATA_DIR)\n    db_path_full = data_path / DB_PATH if not Path(DB_PATH).is_absolute() else DB_PATH\n    \n    storage = SQLiteStorage(str(db_path_full))\n    loader = HistoricalDataLoader(storage)\n    \n    print(\"Loading training data...\")\n    training_data = loader.load_historical_player_logs(\n        start_date=TRAIN_START,\n        end_date=TRAIN_END,\n        num_seasons=1\n    )\n    print(f\"  Loaded {len(training_data)} records\")\n    \n    print(\"Building features...\")\n    feature_config = load_feature_config(FEATURE_CONFIG)\n    pipeline = feature_config.build_pipeline(FeaturePipeline)\n    \n    df = training_data.copy()\n    df['gameDate'] = pd.to_datetime(df['gameDate'], format='%Y%m%d', errors='coerce')\n    df = df.sort_values(['playerID', 'gameDate'])\n    \n    if 'fpts' not in df.columns:\n        df['fpts'] = df.apply(calculate_dk_fantasy_points, axis=1)\n    \n    df['target'] = df.groupby('playerID')['fpts'].shift(-1)\n    train_features = pipeline.fit_transform(df)\n    train_features = train_features.dropna(subset=['target'])\n    \n    metadata_cols = [\n        'playerID', 'playerName', 'longName', 'team', 'teamAbv', 'teamID',\n        'pos', 'gameDate', 'gameID', 'fpts', 'fantasyPoints', 'fantasyPts',\n        'target', 'pts', 'reb', 'ast', 'stl', 'blk', 'TOV', 'mins',\n        'tech', 'created_at', 'updated_at'\n    ]\n    feature_cols = [col for col in train_features.columns if col not in metadata_cols]\n    \n    X_full = train_features[feature_cols].fillna(0)\n    y_full = train_features['target']\n    \n    if len(X_full) > OPTIMIZATION_SAMPLE_SIZE:\n        print(f\"Sampling {OPTIMIZATION_SAMPLE_SIZE} records from {len(X_full)} for optimization...\")\n        sample_idx = np.random.choice(len(X_full), OPTIMIZATION_SAMPLE_SIZE, replace=False)\n        X_sample = X_full.iloc[sample_idx]\n        y_sample = y_full.iloc[sample_idx]\n    else:\n        X_sample = X_full\n        y_sample = y_full\n    \n    print(f\"Optimization dataset: {len(X_sample)} samples, {len(feature_cols)} features\")\n    print()\n    \n    class EarlyStoppingCallback:\n        def __init__(self, patience=10, min_delta=0.01):\n            self.patience = patience\n            self.min_delta = min_delta\n            self.best_value = float('inf')\n            self.trials_without_improvement = 0\n        \n        def __call__(self, study, trial):\n            current_value = trial.value\n            \n            if current_value < self.best_value - self.min_delta:\n                self.best_value = current_value\n                self.trials_without_improvement = 0\n            else:\n                self.trials_without_improvement += 1\n            \n            if self.trials_without_improvement >= self.patience:\n                print(f\"\\nEarly stopping triggered after {trial.number + 1} trials\")\n                print(f\"No improvement for {self.patience} consecutive trials\")\n                print(f\"Best MAPE: {self.best_value:.2f}%\")\n                study.stop()\n    \n    early_stopping = EarlyStoppingCallback(patience=OPTIMIZATION_EARLY_STOPPING_PATIENCE)\n    \n    study = optuna.create_study(\n        direction='minimize',\n        sampler=optuna.samplers.TPESampler(seed=42)\n    )\n    \n    print(f\"Running optimization: {OPTIMIZATION_N_TRIALS} trials, {OPTIMIZATION_TIMEOUT}s timeout\")\n    print(f\"Early stopping: patience={OPTIMIZATION_EARLY_STOPPING_PATIENCE} trials\")\n    print(\"=\"*80)\n    \n    study.optimize(\n        lambda trial: objective(trial, X_sample, y_sample),\n        n_trials=OPTIMIZATION_N_TRIALS,\n        timeout=OPTIMIZATION_TIMEOUT,\n        callbacks=[early_stopping],\n        show_progress_bar=True\n    )\n    \n    print()\n    print(\"=\"*80)\n    print(\"OPTIMIZATION COMPLETE\")\n    print(\"=\"*80)\n    print(f\"Best trial: #{study.best_trial.number}\")\n    print(f\"Best MAPE: {study.best_value:.2f}%\")\n    print(f\"Total trials completed: {len(study.trials)}\")\n    print()\n    print(\"Best hyperparameters:\")\n    for key, value in study.best_params.items():\n        print(f\"  {key}: {value}\")\n    print()\n    \n    OPTIMIZED_MODEL_PARAMS = {\n        **study.best_params,\n        'objective': 'reg:squarederror',\n        'random_state': 42\n    }\n    \n    print(\"MODEL_PARAMS will be updated with optimized values\")\n    \nelse:\n    print(\"Bayesian optimization skipped (RUN_OPTIMIZATION = False)\")\n    print(\"Using default MODEL_PARAMS from section 6d\")\n    OPTIMIZED_MODEL_PARAMS = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6d. Visualize Optimization Results\n",
    "\n",
    "Display interactive visualizations of the optimization process:\n",
    "- **Optimization History**: MAPE over trials with best value tracking\n",
    "- **Parameter Importance**: Which hyperparameters matter most\n",
    "- **Best Trial Parameters**: Optimal hyperparameter values\n",
    "- **Hyperparameter Relationships**: 2D projection of search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTIMIZATION and 'study' in locals():\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            '<b>Optimization History</b>',\n",
    "            '<b>Parameter Importance</b>',\n",
    "            '<b>Best Trial Parameters</b>',\n",
    "            '<b>Hyperparameter Relationships</b>'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{'type': 'scatter'}, {'type': 'bar'}],\n",
    "            [{'type': 'bar'}, {'type': 'scatter'}]\n",
    "        ],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    trials_df = study.trials_dataframe()\n",
    "    \n",
    "    # Optimization history\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trials_df['number'],\n",
    "            y=trials_df['value'],\n",
    "            mode='lines+markers',\n",
    "            name='Trial MAPE',\n",
    "            line=dict(color='#00D7FF', width=2),\n",
    "            marker=dict(size=6, color='#00D7FF')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    best_value_so_far = trials_df['value'].cummin()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trials_df['number'],\n",
    "            y=best_value_so_far,\n",
    "            mode='lines',\n",
    "            name='Best MAPE',\n",
    "            line=dict(color='#9AFF6E', width=3)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Parameter importance\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    param_names = list(importance.keys())\n",
    "    param_values = list(importance.values())\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=param_values,\n",
    "            y=param_names,\n",
    "            orientation='h',\n",
    "            marker=dict(color='#FF53A1', opacity=0.8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    param_names_best = list(best_params.keys())\n",
    "    param_values_best = [str(v) if isinstance(v, (int, float)) else v for v in best_params.values()]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=param_names_best,\n",
    "            x=[1]*len(param_names_best),\n",
    "            orientation='h',\n",
    "            text=param_values_best,\n",
    "            textposition='inside',\n",
    "            marker=dict(color='#18FF6D', opacity=0.8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter relationships (learning_rate vs max_depth)\n",
    "    if 'params_learning_rate' in trials_df.columns and 'params_max_depth' in trials_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trials_df['params_learning_rate'],\n",
    "                y=trials_df['params_max_depth'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=trials_df['value'],\n",
    "                    colorscale='RdYlGn_r',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title='MAPE', x=1.15)\n",
    "                ),\n",
    "                text=[f\"Trial {i}<br>MAPE: {v:.2f}%\" \n",
    "                      for i, v in zip(trials_df['number'], trials_df['value'])],\n",
    "                hoverinfo='text',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_xaxes(title_text='<b>Trial Number</b>', row=1, col=1, color='white')\n",
    "    fig.update_yaxes(title_text='<b>MAPE (%)</b>', row=1, col=1, color='white')\n",
    "    fig.update_xaxes(title_text='<b>Importance</b>', row=1, col=2, color='white')\n",
    "    fig.update_yaxes(title_text='<b>Parameter</b>', row=1, col=2, color='white')\n",
    "    fig.update_xaxes(row=2, col=1, showticklabels=False)\n",
    "    fig.update_yaxes(title_text='<b>Parameter</b>', row=2, col=1, color='white')\n",
    "    fig.update_xaxes(title_text='<b>Learning Rate</b>', row=2, col=2, color='white')\n",
    "    fig.update_yaxes(title_text='<b>Max Depth</b>', row=2, col=2, color='white')\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        plot_bgcolor='#22272e',\n",
    "        paper_bgcolor='#22272e',\n",
    "        font=dict(color='white', size=12),\n",
    "        title_text='<b>Bayesian Optimization Results</b>',\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nOptimization Summary:\")\n",
    "    print(f\"  Total trials: {len(study.trials)}\")\n",
    "    print(f\"  Best trial: #{study.best_trial.number}\")\n",
    "    print(f\"  Best MAPE: {study.best_value:.2f}%\")\n",
    "    print(f\"  Improvement from first trial: {trials_df['value'].iloc[0] - study.best_value:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No optimization results to visualize\")\n",
    "    print(\"Set RUN_OPTIMIZATION = True in section 6a to enable optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Backtest Parameters\n",
    "\n",
    "Set training/testing periods, model configuration, and backtest settings.\n",
    "\n",
    "### Key Parameters:\n",
    "- **Training Period**: Historical data for model training\n",
    "- **Testing Period**: Out-of-sample evaluation dates\n",
    "- **MODEL_PARAMS**: Uses optimized parameters if optimization was run, otherwise uses defaults\n",
    "- **RECALIBRATE_DAYS**: How often to retrain models (default: 7 days)\n",
    "- **SALARY_TIERS**: Bins for salary-based performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Resume Capability\n\nThe backtest now supports incremental saving and resume functionality:\n\n### Features:\n- **Checkpoint Saving**: After each slate completes, results are saved to checkpoints directory\n- **Automatic Resume**: If run is interrupted, set RESUME_FROM_RUN to the timestamp and re-run\n- **Progress Tracking**: progress.json file tracks completed slates and timestamps\n- **Predictions Saved**: Each slate's predictions and actuals saved as parquet files\n\n### How to Resume:\n1. Run cell 8a to check existing run progress\n2. Copy the run timestamp you want to resume (e.g., '20250205_143022')\n3. Set RESUME_FROM_RUN = '20250205_143022' in section 7\n4. Re-run from section 9 onwards\n5. Backtest will skip completed slates and continue from where it left off\n\n### What Gets Saved:\n- Predictions: `outputs/{timestamp}/predictions/{date}.parquet`\n- Predictions with actuals: `outputs/{timestamp}/predictions/{date}_with_actuals.parquet`\n- Checkpoints: `outputs/{timestamp}/checkpoints/{date}.json`\n- Progress: `outputs/{timestamp}/checkpoints/progress.json`\n- Models: `data/models/per_player/` (per-player) or `data/models/per_slate/` (slate-level)\n- Training inputs: `outputs/{timestamp}/inputs/`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "TRAIN_START = '20241001'\nTRAIN_END = '20250204'\nTEST_START = '20250205'\nTEST_END = '20250430'\n\nMODEL_TYPE = 'xgboost'\nFEATURE_CONFIG = 'default_features'\nMIN_PLAYER_GAMES = 10\nMIN_GAMES_FOR_BENCHMARK = 5\nRECALIBRATE_DAYS = 7\nREWRITE_MODELS = False\nSALARY_TIERS = [0, 4000, 6000, 8000, 15000]\n\nN_JOBS = -1\n\nDB_PATH = 'nba_dfs.db'\nDATA_DIR = '/content/delapan-fantasy/MyDrive/dfs/data'\nOUTPUT_DIR = 'outputs'\n\nRESUME_FROM_RUN = None\n\nif 'OPTIMIZED_MODEL_PARAMS' in locals() and OPTIMIZED_MODEL_PARAMS is not None:\n    MODEL_PARAMS = OPTIMIZED_MODEL_PARAMS\n    print(\"Using OPTIMIZED hyperparameters from Bayesian optimization\")\nelse:\n    MODEL_PARAMS = {\n        'max_depth': 6,\n        'learning_rate': 0.05,\n        'n_estimators': 200,\n        'min_child_weight': 5,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'objective': 'reg:squarederror',\n        'random_state': 42\n    }\n    print(\"Using DEFAULT hyperparameters (optimization not run)\")\n\nprint(\"\\nBacktest Configuration:\")\nprint(f\"  Training: {TRAIN_START} to {TRAIN_END}\")\nprint(f\"  Testing: {TEST_START} to {TEST_END}\")\nprint(f\"  Model: {MODEL_TYPE}\")\nprint(f\"  Features: {FEATURE_CONFIG}\")\nprint(f\"  Min games: {MIN_PLAYER_GAMES}\")\nprint(f\"  Benchmark min games: {MIN_GAMES_FOR_BENCHMARK}\")\nprint(f\"  Recalibrate: every {RECALIBRATE_DAYS} days\")\nprint(f\"  Rewrite models: {REWRITE_MODELS}\")\nprint(f\"  Parallel jobs: {N_JOBS} (all cores)\")\nprint(f\"  Salary tiers: {SALARY_TIERS}\")\nprint(f\"  Resume from run: {RESUME_FROM_RUN if RESUME_FROM_RUN else 'None (fresh start)'}\")\nprint(f\"\\nPaths (Separated Architecture):\")\nprint(f\"  Data directory: {DATA_DIR}\")\nprint(f\"  Database: {DATA_DIR}/{DB_PATH}\")\nprint(f\"  Output: {DATA_DIR}/{OUTPUT_DIR}\")\nprint(f\"\\nMODEL_PARAMS:\")\nfor key, value in MODEL_PARAMS.items():\n    print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Import Dependencies\n\nImport core backtest framework and configure logging.\n\n**Important**: Logging set to WARNING level to prevent notebook lag. With 500+ models being trained, INFO-level logging generates thousands of messages that freeze Colab's output rendering."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nimport pandas as pd\nfrom datetime import datetime\n\nfrom src.walk_forward_backtest import WalkForwardBacktest\n\n# Reduce logging verbosity to prevent notebook lag\nlogging.basicConfig(\n    level=logging.WARNING,  # Changed from INFO to WARNING\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Suppress verbose library logs\nlogging.getLogger('src').setLevel(logging.WARNING)\nlogging.getLogger('xgboost').setLevel(logging.ERROR)\nlogging.getLogger('optuna').setLevel(logging.WARNING)\n\nprint(\"Imports successful - Logging set to WARNING level to prevent lag\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Backtest\n",
    "\n",
    "Create WalkForwardBacktest instance with configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "source": "backtest = WalkForwardBacktest(\n    db_path=DB_PATH,\n    data_dir=DATA_DIR,\n    train_start=TRAIN_START,\n    train_end=TRAIN_END,\n    test_start=TEST_START,\n    test_end=TEST_END,\n    model_type=MODEL_TYPE,\n    model_params=MODEL_PARAMS,\n    feature_config=FEATURE_CONFIG,\n    output_dir=OUTPUT_DIR,\n    per_player_models=True,\n    min_player_games=MIN_PLAYER_GAMES,\n    min_games_for_benchmark=MIN_GAMES_FOR_BENCHMARK,\n    recalibrate_days=RECALIBRATE_DAYS,\n    num_seasons=1,\n    salary_tiers=SALARY_TIERS,\n    rewrite_models=REWRITE_MODELS,\n    save_models=True,\n    save_predictions=True,\n    n_jobs=N_JOBS,\n    resume_from_run=RESUME_FROM_RUN\n)\n\nprint(\"Backtest initialized with separated architecture\")\nif RESUME_FROM_RUN:\n    print(f\"Will resume from run: {RESUME_FROM_RUN}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8a. Check Existing Run Progress (Optional)\n\nUse this cell to check progress of existing runs and set RESUME_FROM_RUN if you want to resume.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest = WalkForwardBacktest(\n",
    "    db_path=DB_PATH,\n",
    "    data_dir=DATA_DIR,\n",
    "    train_start=TRAIN_START,\n",
    "    train_end=TRAIN_END,\n",
    "    test_start=TEST_START,\n",
    "    test_end=TEST_END,\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_params=MODEL_PARAMS,\n",
    "    feature_config=FEATURE_CONFIG,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_player_models=True,\n",
    "    min_player_games=MIN_PLAYER_GAMES,\n",
    "    min_games_for_benchmark=MIN_GAMES_FOR_BENCHMARK,\n",
    "    recalibrate_days=RECALIBRATE_DAYS,\n",
    "    num_seasons=1,\n",
    "    salary_tiers=SALARY_TIERS,\n",
    "    rewrite_models=REWRITE_MODELS,\n",
    "    save_models=True,\n",
    "    save_predictions=True,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(\"Backtest initialized with separated architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Backtest\n",
    "\n",
    "Execute walk-forward backtest across all test dates.\n",
    "\n",
    "**Time estimate per slate**:\n",
    "- Free Colab: ~21 minutes\n",
    "- Colab Pro: ~10 minutes\n",
    "- Colab Pro+: ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f\"Starting backtest at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = backtest.run()\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed = end_time - start_time\n",
    "print(\"=\"*80)\n",
    "print(f\"Backtest completed at {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. View Results Summary\n",
    "\n",
    "Display aggregated performance metrics and statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BACKTEST RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Slates processed: {results['num_slates']}\")\n",
    "print(f\"Date range: {results['date_range']}\")\n",
    "print(f\"Total players evaluated: {results['total_players_evaluated']:.0f}\")\n",
    "print(f\"Average players per slate: {results['avg_players_per_slate']:.1f}\")\n",
    "print()\n",
    "print(\"Model Performance:\")\n",
    "print(f\"  Mean MAPE: {results['model_mean_mape']:.2f}%\")\n",
    "print(f\"  Median MAPE: {results['model_median_mape']:.2f}%\")\n",
    "print(f\"  Std MAPE: {results['model_std_mape']:.2f}%\")\n",
    "print(f\"  Mean RMSE: {results['model_mean_rmse']:.2f}\")\n",
    "print(f\"  Mean MAE: {results['model_mean_mae']:.2f}\")\n",
    "print(f\"  Mean Correlation: {results['model_mean_correlation']:.3f}\")\n",
    "print()\n",
    "print(\"Benchmark Performance:\")\n",
    "print(f\"  Mean MAPE: {results['benchmark_mean_mape']:.2f}%\")\n",
    "print(f\"  Improvement: {results['mape_improvement']:+.2f}%\")\n",
    "print()\n",
    "if 'statistical_test' in results:\n",
    "    test = results['statistical_test']\n",
    "    print(\"Statistical Significance:\")\n",
    "    print(f\"  p-value: {test['p_value']:.6f}\")\n",
    "    print(f\"  Cohen's d: {test['cohens_d']:.4f}\")\n",
    "    print(f\"  Effect size: {test['effect_size']}\")\n",
    "    if test['p_value'] < 0.05:\n",
    "        status = \"Model BETTER\" if test['cohens_d'] < 0 else \"Model WORSE\"\n",
    "        print(f\"  Result: {status} (statistically significant)\")\n",
    "    else:\n",
    "        print(f\"  Result: No significant difference\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Daily Results\n",
    "\n",
    "Examine per-slate performance breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = results['daily_results']\n",
    "\n",
    "print(\"Daily Results:\")\n",
    "print(\"=\"*80)\n",
    "display(daily_df[[\n",
    "    'date', 'num_players', 'model_mape', 'model_rmse', 'model_mae',\n",
    "    'model_corr', 'benchmark_mape', 'mean_actual', 'mean_projected'\n",
    "]])\n",
    "\n",
    "if 'tier_comparison' in results:\n",
    "    print(\"\\n\\nPerformance by Salary Tier:\")\n",
    "    print(\"=\"*80)\n",
    "    tier_df = results['tier_comparison']\n",
    "    display(tier_df[['salary_tier', 'count', 'model_mape', 'benchmark_mape', 'mape_improvement']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Results\n",
    "\n",
    "Interactive charts showing:\n",
    "- MAPE over time (model vs benchmark)\n",
    "- RMSE progression\n",
    "- Correlation trends\n",
    "- Players evaluated per slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "vibrant_colors = {\n",
    "    \"model\": \"#9AFF6E\",\n",
    "    \"benchmark\": \"#3ABEFF\", \n",
    "    \"model_mean\": \"#FFFF35\",\n",
    "    \"benchmark_mean\": \"#FD5A66\",\n",
    "    \"rmse\": \"#00FFC2\",\n",
    "    \"rmse_mean\": \"#EA00FF\",\n",
    "    \"corr\": \"#FBBF24\",\n",
    "    \"corr_mean\": \"#FF7A00\",\n",
    "    \"players_bar\": \"#FF53A1\",\n",
    "    \"players_mean\": \"#25FFF1\",\n",
    "}\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        '<b>MAPE Over Time</b>', \n",
    "        '<b>RMSE Over Time</b>', \n",
    "        '<b>Correlation Over Time</b>', \n",
    "        '<b>Players Evaluated Per Slate</b>'\n",
    "    ),\n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# MAPE Over Time\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_df['date'], \n",
    "        y=daily_df['model_mape'], \n",
    "        mode='lines+markers', \n",
    "        name='Model', \n",
    "        line=dict(width=2, color=vibrant_colors[\"model\"]), \n",
    "        marker=dict(size=10, color=vibrant_colors[\"model\"], symbol='circle')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_df['date'], \n",
    "        y=daily_df['benchmark_mape'], \n",
    "        mode='lines+markers', \n",
    "        name='Benchmark', \n",
    "        line=dict(width=2, color=vibrant_colors[\"benchmark\"]),\n",
    "        marker=dict(size=10, color=vibrant_colors[\"benchmark\"], symbol='square')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=daily_df['model_mape'].mean(),\n",
    "    line_dash=\"dash\",\n",
    "    line_color=vibrant_colors[\"model_mean\"],\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=daily_df['benchmark_mape'].mean(),\n",
    "    line_dash=\"dash\",\n",
    "    line_color=vibrant_colors[\"benchmark_mean\"],\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# RMSE Over Time  \n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_df['date'], \n",
    "        y=daily_df['model_rmse'], \n",
    "        mode='lines+markers', \n",
    "        name='RMSE',\n",
    "        line=dict(color=vibrant_colors[\"rmse\"], width=2), \n",
    "        marker=dict(size=10, color=vibrant_colors[\"rmse\"], symbol='diamond')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=daily_df['model_rmse'].mean(), \n",
    "    line_dash=\"dash\", \n",
    "    line_color=vibrant_colors[\"rmse_mean\"], \n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Correlation Over Time\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_df['date'], \n",
    "        y=daily_df['model_corr'], \n",
    "        mode='lines+markers', \n",
    "        name='Correlation',\n",
    "        line=dict(color=vibrant_colors[\"corr\"], width=2), \n",
    "        marker=dict(size=10, color=vibrant_colors[\"corr\"], symbol='cross')\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=daily_df['model_corr'].mean(), \n",
    "    line_dash=\"dash\", \n",
    "    line_color=vibrant_colors[\"corr_mean\"], \n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Players Evaluated\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=daily_df['date'], \n",
    "        y=daily_df['num_players'], \n",
    "        name='Players', \n",
    "        marker=dict(color=vibrant_colors[\"players_bar\"], opacity=0.85)\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=daily_df['num_players'].mean(), \n",
    "    line_dash=\"dash\", \n",
    "    line_color=vibrant_colors[\"players_mean\"], \n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "axis_style = dict(color=\"white\", showline=True, linewidth=1.5, linecolor='#666')\n",
    "fig.update_xaxes(title_text=\"<b>Date</b>\", tickangle=45, **axis_style)\n",
    "fig.update_yaxes(title_text=\"<b>MAPE (%)</b>\", row=1, col=1, **axis_style)\n",
    "fig.update_yaxes(title_text=\"<b>RMSE</b>\", row=1, col=2, **axis_style)\n",
    "fig.update_yaxes(title_text=\"<b>Correlation</b>\", row=2, col=1, **axis_style)\n",
    "fig.update_yaxes(title_text=\"<b>Players</b>\", row=2, col=2, **axis_style)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"center\", x=0.5),\n",
    "    plot_bgcolor=\"#22272e\",\n",
    "    paper_bgcolor=\"#22272e\",\n",
    "    font=dict(color=\"white\", size=14),\n",
    "    title_text=\"<b>Backtest Performance Metrics</b>\",\n",
    "    title_x=0.5,\n",
    "    margin=dict(t=110)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Error Distribution Analysis\n",
    "\n",
    "Compare model errors vs benchmark errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'all_predictions' in results and not results['all_predictions'].empty:\n",
    "    all_preds = results['all_predictions']\n",
    "    comparison_df = all_preds[(all_preds['projected_fpts'] > 0) & (all_preds['benchmark_pred'] > 0)].copy()\n",
    "    \n",
    "    vibrant_colors_error = {\n",
    "        \"scatter\": \"#00D7FF\",\n",
    "        \"diagonal\": \"#FF0080\",\n",
    "        \"histogram\": \"#18FF6D\",\n",
    "        \"vline_zero\": \"#FF0080\",\n",
    "        \"vline_mean\": \"#FFD700\",\n",
    "    }\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\n",
    "            '<b>Model vs Benchmark Error</b>', \n",
    "            '<b>Error Difference (Positive = Model Better)</b>'\n",
    "        ),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    comparison_df['model_error'] = abs(comparison_df['projected_fpts'] - comparison_df['actual_fpts'])\n",
    "    comparison_df['benchmark_error'] = abs(comparison_df['benchmark_pred'] - comparison_df['actual_fpts'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=comparison_df['benchmark_error'], \n",
    "            y=comparison_df['model_error'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=7, opacity=0.7, color=vibrant_colors_error[\"scatter\"]),\n",
    "            name='Errors'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    max_error = max(comparison_df['benchmark_error'].max(), comparison_df['model_error'].max()) * 1.03\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, max_error], y=[0, max_error],\n",
    "            mode='lines',\n",
    "            line=dict(color=vibrant_colors_error[\"diagonal\"], dash='dash', width=2),\n",
    "            name='Equal error'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    error_diff = comparison_df['benchmark_error'] - comparison_df['model_error']\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=error_diff, nbinsx=30,\n",
    "            marker=dict(color=vibrant_colors_error[\"histogram\"], opacity=0.85),\n",
    "            name='Error Difference'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_vline(x=0, line_dash=\"dash\", line_color=vibrant_colors_error[\"vline_zero\"], row=1, col=2)\n",
    "    fig.add_vline(x=error_diff.mean(), line_dash=\"dash\", line_color=vibrant_colors_error[\"vline_mean\"], row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"<b>Benchmark Error</b>\", row=1, col=1, color=\"white\")\n",
    "    fig.update_yaxes(title_text=\"<b>Model Error</b>\", row=1, col=1, color=\"white\")\n",
    "    fig.update_xaxes(title_text=\"<b>Error Difference</b>\", row=1, col=2, color=\"white\")\n",
    "    fig.update_yaxes(title_text=\"<b>Frequency</b>\", row=1, col=2, color=\"white\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        plot_bgcolor=\"#22272e\",\n",
    "        paper_bgcolor=\"#22272e\",\n",
    "        font=dict(color=\"white\", size=14),\n",
    "        title_text=\"<b>Error Distribution Analysis</b>\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nError Statistics:\")\n",
    "    print(f\"  Model Mean Error: {comparison_df['model_error'].mean():.2f}\")\n",
    "    print(f\"  Benchmark Mean Error: {comparison_df['benchmark_error'].mean():.2f}\")\n",
    "    print(f\"  Mean Difference: {error_diff.mean():.2f} (positive = model better)\")\n",
    "else:\n",
    "    print(\"No prediction data available for error analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Salary Tier Performance\n",
    "\n",
    "Analyze model performance across different salary ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tier_comparison' in results:\n",
    "    tier_df = results['tier_comparison']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('<b>MAPE by Salary Tier</b>', '<b>Model Improvement Over Benchmark</b>'),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    x_labels = tier_df['salary_tier'].astype(str)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=x_labels, y=tier_df['model_mape'],\n",
    "               name='Model', marker=dict(opacity=0.8, color='#9AFF6E')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=x_labels, y=tier_df['benchmark_mape'],\n",
    "               name='Benchmark', marker=dict(opacity=0.8, color='#3ABEFF')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    colors = ['green' if x > 0 else 'red' for x in tier_df['mape_improvement']]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=x_labels, y=tier_df['mape_improvement'],\n",
    "               marker=dict(color=colors, opacity=0.7),\n",
    "               showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_hline(y=0, line_color=\"white\", line_width=0.8, row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"<b>Salary Tier</b>\", color=\"white\")\n",
    "    fig.update_yaxes(title_text=\"<b>MAPE (%)</b>\", row=1, col=1, color=\"white\")\n",
    "    fig.update_yaxes(title_text=\"<b>MAPE Improvement (%)</b>\", row=1, col=2, color=\"white\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        barmode='group',\n",
    "        plot_bgcolor=\"#22272e\",\n",
    "        paper_bgcolor=\"#22272e\",\n",
    "        font=dict(color=\"white\", size=14),\n",
    "        title_text=\"<b>Performance by Salary Tier</b>\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nSalary Tier Breakdown:\")\n",
    "    print(\"=\"*80)\n",
    "    for _, row in tier_df.iterrows():\n",
    "        improvement = row['mape_improvement']\n",
    "        status = 'BETTER' if improvement > 0 else 'WORSE'\n",
    "        print(f\"{str(row['salary_tier']):20} {improvement:+6.1f}% {status:8} \"\n",
    "              f\"(Model: {row['model_mape']:.1f}%, Benchmark: {row['benchmark_mape']:.1f}%, n={row['count']:.0f})\")\n",
    "else:\n",
    "    print(\"No tier comparison data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Results\n",
    "\n",
    "Export summary CSV and tier comparison to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if 'report_path' in results:\n",
    "    print(f\"Comprehensive report: {results['report_path']}\")\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {backtest.run_output_dir}\")\n",
    "print(f\"  - Predictions: {backtest.run_predictions_dir}\")\n",
    "print(f\"  - Training inputs: {backtest.run_inputs_dir}\")\n",
    "print(f\"  - Features: {backtest.run_features_dir}\")\n",
    "\n",
    "output_files = list(Path(backtest.run_output_dir).rglob('*.parquet'))\n",
    "json_files = list(Path(backtest.run_output_dir).rglob('*.json'))\n",
    "pkl_files = list(Path(backtest.run_output_dir).rglob('*.pkl'))\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - Parquet: {len(output_files)}\")\n",
    "print(f\"  - JSON: {len(json_files)}\")\n",
    "print(f\"  - Models: {len(pkl_files)}\")\n",
    "print(f\"  - Total: {len(output_files) + len(json_files) + len(pkl_files)}\")\n",
    "\n",
    "# Save summary CSV\n",
    "date_range_clean = results['date_range'].replace(' to ', '_')\n",
    "summary_path = Path(DATA_DIR) / 'outputs' / f\"summary_{date_range_clean}.csv\"\n",
    "daily_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSummary CSV: {summary_path}\")\n",
    "\n",
    "# Save tier comparison if available\n",
    "if 'tier_comparison' in results:\n",
    "    tier_path = Path(DATA_DIR) / 'outputs' / f\"tier_comparison_{date_range_clean}.csv\"\n",
    "    results['tier_comparison'].to_csv(tier_path, index=False)\n",
    "    print(f\"Tier comparison: {tier_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}